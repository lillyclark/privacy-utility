{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_x(x):\n",
    "    for col in range(x.shape[1]):\n",
    "        vals = x[:,col]\n",
    "        vals = (vals-np.mean(vals))/np.std(vals)\n",
    "        x[:,col] = vals\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleChania(Chania, n_points):\n",
    "    data = Chania[:,0:8]\n",
    "    np.random.seed(0)\n",
    "    order = np.argsort(np.random.random(data.shape[0]))\n",
    "    x = data[order][:n_points, 1:5]   \n",
    "    trueusers = data[order][:n_points,0]\n",
    "    n_clusters = 5\n",
    "    x = norm_x(x)\n",
    "    return tf.constant(x, dtype=tf.float32), tf.constant(trueusers, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chania = np.genfromtxt('processedChania.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privitizer and Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privatizer(x):\n",
    "    with tf.variable_scope(\"priv\", reuse=tf.AUTO_REUSE):\n",
    "        l1 = tf.layers.dense(x, 8, activation=tf.nn.relu)\n",
    "        l2 = tf.layers.dense(l1, x.shape[1].value)\n",
    "    return l2\n",
    "\n",
    "\n",
    "def adversary(y):\n",
    "    with tf.variable_scope(\"ad\", reuse=tf.AUTO_REUSE):\n",
    "        l1 = tf.layers.dense(y, 16, activation=tf.nn.relu)\n",
    "        # num users = 5\n",
    "        l2 = tf.layers.dense(l1, 5, activation=tf.nn.softmax)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Map Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_map_error(x, y):\n",
    "    with tf.variable_scope(\"priv\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "        # build model from input\n",
    "        inputmodel = keras.Sequential([\n",
    "            keras.layers.Dense(10, input_shape=(2, ), activation=tf.nn.relu),\n",
    "            keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "            keras.layers.Dense(1)        \n",
    "        ])\n",
    "        optimizer = tf.train.FtrlOptimizer(0.001)\n",
    "        inputmodel.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "        inputmodel.fit(x[:,2:], x[:,1], steps_per_epoch=1, epochs=100, verbose=0)   \n",
    "\n",
    "        # build model from output\n",
    "        outputmodel = keras.Sequential([\n",
    "            keras.layers.Dense(10, input_shape=(2, ), activation=tf.nn.relu),\n",
    "            keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "            keras.layers.Dense(1)        \n",
    "        ])\n",
    "        outputmodel.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "        outputmodel.fit(y[:,2:], y[:,1], steps_per_epoch=1, epochs=100, verbose=0)  \n",
    "        \n",
    "        # compare inputmodel(input) and outputmodel(input)\n",
    "        # map_error = tf.reduce_mean(tf.squared_difference(inputmodel(x[:,2:]), outputmodel(x[:,2:])))\n",
    "        \n",
    "        # compare inputmodel(testpoints) and outputmodel(testpoints)\n",
    "        testpoints = np.mgrid[-1:1:15j, -1:1:15j].reshape(2,-1).T\n",
    "        grid = tf.constant(testpoints, dtype=tf.float32)\n",
    "        map_error = tf.reduce_mean(tf.squared_difference(inputmodel(grid), outputmodel(grid)))\n",
    "\n",
    "    return map_error, inputmodel, outputmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(z, trueusers):\n",
    "    with tf.variable_scope(\"ad\", reuse=tf.AUTO_REUSE):\n",
    "        class_error = tf.reduce_mean(tf.keras.backend.sparse_categorical_crossentropy(trueusers, z))\n",
    "    return class_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privatizer Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privatizer_loss(x, y, trueusers, threshold):\n",
    "    with tf.variable_scope(\"priv\", reuse=tf.AUTO_REUSE):\n",
    "        penalty = 2\n",
    "        \n",
    "        c_e = classification_error(z, trueusers)\n",
    "        s_m_e, _, _ = signal_map_error(x, y)\n",
    "        \n",
    "        zero = tf.constant(0, dtype=tf.float32)\n",
    "        loss = -1*c_e + penalty*tf.math.maximum(zero, threshold-s_m_e)\n",
    "        \n",
    "    return loss        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize privatizer and adversary\n",
    "# for i in range(n):\n",
    "#     choose sample points\n",
    "#     y = privatizer(x)\n",
    "#     z = adversary(y)\n",
    "#     for k in range(100):\n",
    "#         train adversary\n",
    "#     train privatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Adversary loss: 1.7233\t Privatizer loss: -0.9997\n"
     ]
    }
   ],
   "source": [
    "# initialize \n",
    "threshold = tf.constant(0.5, dtype=tf.float32)\n",
    "x, trueusers = sampleChania(Chania, n_points=100)\n",
    "y = privatizer(x)\n",
    "z = adversary(y)\n",
    "n = 1\n",
    "\n",
    "# initialize loss variables\n",
    "class_error = classification_error(z, trueusers)\n",
    "priv_loss = privatizer_loss(x, y, trueusers, threshold)\n",
    "\n",
    "# initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    # sample points\n",
    "    x, trueusers = sampleChania(Chania, n_points=100)\n",
    "    \n",
    "    # run data through privatizer\n",
    "    y = privatizer(x)\n",
    "    \n",
    "    # run data through adversary\n",
    "    z = adversary(y)\n",
    "\n",
    "    ad_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"ad\")\n",
    "    ad_train = tf.train.GradientDescentOptimizer(0.01).minimize(class_error, var_list = ad_vars)\n",
    "    \n",
    "    priv_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"priv\")\n",
    "    priv_train = tf.train.GradientDescentOptimizer(0.01).minimize(priv_loss, var_list = priv_vars)\n",
    "    \n",
    "    for k in range(1):\n",
    "        \n",
    "        # train adversary\n",
    "        _, a_loss = sess.run((ad_train, class_error))\n",
    "        \n",
    "    # train privatizer\n",
    "    _, p_loss = sess.run((priv_train, priv_loss))\n",
    "    \n",
    "    print(\"Iterations: %d\\t Adversary loss: %.4f\\t Privatizer loss: %.4f\"%(i, a_loss,p_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rssi(x, y, sess):\n",
    "    xarray = x.eval(session=sess)\n",
    "    yarray = y.eval(session=sess)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    ax[0].scatter(xarray[:,3], xarray[:,2], c=xarray[:,1])\n",
    "    ax[0].set_title(\"True Received Signal Strengths\")\n",
    "    \n",
    "    ax[1].scatter(yarray[:,3], yarray[:,2], c=yarray[:,1])\n",
    "    ax[1].set_title(\"Obfuscated Received Signal Strengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_users(x, y, z, trueusers, sess):\n",
    "    xarray = x.eval(session=sess)\n",
    "    yarray = y.eval(session=sess)\n",
    "    zarray = z.eval(session=sess)\n",
    "    u = trueusers.eval(session=sess)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    ax1[0].scatter(xarray[:,3], xarray[:,2], c=u)\n",
    "    ax1[0].set_title(\"User Labels\")\n",
    "    \n",
    "    ax1[1].scatter(yarray[:,3], yarray[:,2], c=u)\n",
    "    ax1[1].set_title(\"Obfuscated data - User Labels\")\n",
    "    \n",
    "    fig, ax2 = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    ax2[0].scatter(xarray[:,3], xarray[:,2], c=np.argmax(zarray, axis=1))\n",
    "    ax2[0].set_title(\"User Labels estimated by adversary\")\n",
    "    \n",
    "    ax2[1].scatter(yarray[:,3], yarray[:,2], c=np.argmax(zarray, axis=1))\n",
    "    ax2[1].set_title(\"Obfuscated data - User Labels estimated by adversary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps(inputmodel, outputmodel, sess):   \n",
    "    testpoints = np.mgrid[-2:2:15j, -2:2:15j].reshape(2,-1).T\n",
    "    grid = tf.constant(testpoints, dtype=tf.float32)\n",
    "\n",
    "    inputmap = inputmodel.predict_on_batch(grid.eval(session=sess))\n",
    "    outputmap = outputmodel.predict_on_batch(grid.eval(session=sess))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax[0].scatter(testpoints[:,1], testpoints[:,0], c=inputmap[:,0])\n",
    "    ax[0].set_title(\"Map created from True Data\")\n",
    "    ax[1].scatter(testpoints[:,1], testpoints[:,0], c=outputmap[:,0])\n",
    "    ax[1].set_title(\"Map created from Obfuscated Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plot_rssi(x, y, sess)\n",
    "    plot_users(x, y, z, trueusers, sess)\n",
    "    _, inputmodel, outputmodel = signal_map_error(x, y)\n",
    "    plot_maps(inputmodel, outputmodel, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "#### Metaparameters to consider playing with:\n",
    "1. number of layers & units in privatizer and adversary\n",
    "2. number of layers and & units in signal map models\n",
    "3. penalty\n",
    "4. threshold\n",
    "5. number of points to train on each iteration\n",
    "6. ratio of adversary training steps to privatizer training steps\n",
    "\n",
    "#### Additional note:  \n",
    "I am calculating signal map error as the mean square error of the ouput of two models when they are both fed with the input x. Should I instead be feeding a uniform input of hypothetical points (like the visualization?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAP(privscope, adscope, priv_nlayers, priv_nunits, ad_nlayers, ad_nunits, map_nlayers, map_nunits, map_epochs, penalty_value, threshold_value, samplesize, n_iterations):\n",
    "\n",
    "    f = open((\"train_GAP/\"+privscope+adscope+str((priv_nlayers, priv_nunits, ad_nlayers, ad_nunits, map_nlayers, \n",
    "                                map_nunits, map_epochs, penalty_value, threshold_value, samplesize, \n",
    "                                n_iterations))+\".txt\"), \"w\")\n",
    "    f.write(\"%d layers with %d units in the privatizer\\n\"%(priv_nlayers, priv_nunits))\n",
    "    f.write(\"%d layers with %d units in the adversary\\n\"%(ad_nlayers, ad_nunits))\n",
    "    f.write(\"%d layers with %d units in the map model, trained for %d epochs\\n\"%(map_nlayers, map_nunits, map_epochs))\n",
    "    f.write(\"utility distortion penalty of %d\\n\"%(penalty_value))\n",
    "    f.write(\"minimum acceptable distortion of %.2f\\n\"%(threshold_value))\n",
    "    f.write(\"train on %d samples\\n\"%(samplesize))\n",
    "    f.write(\"try %d iterations\\n\\n\"%(n_iterations))\n",
    "    \n",
    "    def privatizer(x):\n",
    "        with tf.variable_scope(privscope, reuse=tf.AUTO_REUSE):\n",
    "            l1 = tf.layers.dense(x, priv_nunits, activation=tf.nn.relu)\n",
    "            if priv_nlayers == 1:\n",
    "                l2 = tf.layers.dense(l1, x.shape[1].value)\n",
    "                return l2\n",
    "            if priv_nlayers == 2:\n",
    "                l2 = tf.layers.dense(l1, priv_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, x.shape[1].value)\n",
    "                return l3\n",
    "            if priv_nlayers == 3:\n",
    "                l2 = tf.layers.dense(l1, priv_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, priv_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, x.shape[1].value)\n",
    "                return l4\n",
    "            if priv_nlayers == 4:\n",
    "                l2 = tf.layers.dense(l1, priv_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, priv_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, priv_nunits, activation=tf.nn.relu)\n",
    "                l5 = tf.layers.dense(l4, x.shape[1].value)\n",
    "                return l5\n",
    "            if priv_nlayers == 5:\n",
    "                l2 = tf.layers.dense(l1, priv_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, priv_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, priv_nunits, activation=tf.nn.relu)\n",
    "                l5 = tf.layers.dense(l4, priv_nunits, activation=tf.nn.relu)\n",
    "                l6 = tf.layers.dense(l5, x.shape[1].value)\n",
    "                return l6\n",
    "            else:\n",
    "                raise ValueError(\"Can't handle more than 5 hidden layers right now\")\n",
    "\n",
    "    def adversary(y):\n",
    "        with tf.variable_scope(adscope, reuse=tf.AUTO_REUSE):\n",
    "            l1 = tf.layers.dense(y, ad_nunits, activation=tf.nn.relu)\n",
    "            if ad_nlayers == 1:\n",
    "                l2 = tf.layers.dense(l1, 5, activation=tf.nn.softmax)\n",
    "                return l2\n",
    "            if ad_nlayers == 2:\n",
    "                l2 = tf.layers.dense(l1, ad_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, 5, activation=tf.nn.softmax)\n",
    "                return l3\n",
    "            if ad_nlayers == 3:\n",
    "                l2 = tf.layers.dense(l1, ad_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, ad_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, 5, activation=tf.nn.softmax)\n",
    "                return l4\n",
    "            if ad_nlayers == 4:\n",
    "                l2 = tf.layers.dense(l1, ad_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, ad_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, ad_nunits, activation=tf.nn.relu)\n",
    "                l5 = tf.layers.dense(l4, 5, activation=tf.nn.softmax)\n",
    "                return l5\n",
    "            if ad_nlayers == 5:\n",
    "                l2 = tf.layers.dense(l1, ad_nunits, activation=tf.nn.relu)\n",
    "                l3 = tf.layers.dense(l2, ad_nunits, activation=tf.nn.relu)\n",
    "                l4 = tf.layers.dense(l3, ad_nunits, activation=tf.nn.relu)\n",
    "                l5 = tf.layers.dense(l4, ad_nunits, activation=tf.nn.relu)\n",
    "                l6 = tf.layers.dense(l5, 5, activation=tf.nn.softmax)\n",
    "                return l6\n",
    "            else:\n",
    "                raise ValueError(\"Can't handle more than 5 hidden layers right now\")\n",
    "    \n",
    "    def signal_map_error(x, y):\n",
    "        with tf.variable_scope(privscope, reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # build model from input\n",
    "            inputmodel = keras.Sequential()\n",
    "            inputmodel.add(keras.layers.Dense(map_nunits, input_shape=(2, ), activation=tf.nn.relu))\n",
    "            for l in range(map_nlayers-1):\n",
    "                inputmodel.add(keras.layers.Dense(map_nunits, activation=tf.nn.relu))\n",
    "            inputmodel.add(keras.layers.Dense(1))\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            inputmodel.compile(loss='mse', optimizer=optimizer)\n",
    "            inputmodel.fit(x[:,2:], x[:,1], steps_per_epoch=10, epochs=map_epochs, verbose=0, \n",
    "                           callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001)])\n",
    "\n",
    "            # build model from output\n",
    "            outputmodel = keras.Sequential()\n",
    "            outputmodel.add(keras.layers.Dense(map_nunits, input_shape=(2, ), activation=tf.nn.relu))\n",
    "            for l in range(map_nlayers-1):\n",
    "                outputmodel.add(keras.layers.Dense(map_nunits, activation=tf.nn.relu))\n",
    "            outputmodel.add(keras.layers.Dense(1))\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            outputmodel.compile(loss='mse', optimizer=optimizer)\n",
    "            outputmodel.fit(y[:,2:], y[:,1], steps_per_epoch=10, epochs=map_epochs, verbose=0,\n",
    "                           callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001)])  \n",
    "\n",
    "            # compare inputmodel(input) and outputmodel(input)\n",
    "            # map_error = tf.reduce_mean(tf.squared_difference(inputmodel(x[:,2:]), outputmodel(x[:,2:])))\n",
    "\n",
    "            # compare inputmodel(testpoints) and outputmodel(testpoints)\n",
    "            testpoints = np.mgrid[-2:2:15j, -2:2:15j].reshape(2,-1).T\n",
    "            grid = tf.constant(testpoints, dtype=tf.float32)\n",
    "            map_error = tf.reduce_mean(tf.squared_difference(inputmodel(grid), outputmodel(grid)))\n",
    "\n",
    "        return map_error, inputmodel, outputmodel\n",
    "    \n",
    "    def classification_error(z, trueusers):\n",
    "        with tf.variable_scope(adscope, reuse=tf.AUTO_REUSE):\n",
    "            class_error = tf.reduce_mean(tf.keras.backend.sparse_categorical_crossentropy(trueusers, z))\n",
    "        return class_error\n",
    "    \n",
    "    def privatizer_loss(x, y, trueusers, threshold):\n",
    "        with tf.variable_scope(privscope, reuse=tf.AUTO_REUSE):\n",
    "            penalty = penalty_value\n",
    "\n",
    "            c_e = classification_error(z, trueusers)\n",
    "            s_m_e, _, _ = signal_map_error(x, y)\n",
    "\n",
    "            zero = tf.constant(0, dtype=tf.float32)\n",
    "            loss = -1*c_e + penalty*tf.math.maximum(zero, s_m_e-threshold)\n",
    "\n",
    "        return loss        \n",
    "    \n",
    "    # initialize \n",
    "    threshold = tf.constant(threshold_value, dtype=tf.float32)\n",
    "    x, trueusers = sampleChania(Chania, samplesize)\n",
    "    y = privatizer(x)\n",
    "    z = adversary(y)\n",
    "    n = n_iterations\n",
    "\n",
    "    # initialize loss variables\n",
    "    class_error = classification_error(z, trueusers)\n",
    "    priv_loss = privatizer_loss(x, y, trueusers, threshold)\n",
    "\n",
    "    # initialize session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ad_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=adscope)\n",
    "    ad_train = tf.train.GradientDescentOptimizer(0.01).minimize(class_error, var_list = ad_vars)\n",
    "\n",
    "    priv_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=privscope)\n",
    "    priv_train = tf.train.GradientDescentOptimizer(0.01).minimize(priv_loss, var_list = priv_vars)\n",
    "    \n",
    "    epsilon1 = 0.00001\n",
    "    previous_loss1 = 0\n",
    "    for i in range(n):\n",
    "\n",
    "        x, trueusers = sampleChania(Chania, n_points=100)\n",
    "        y = privatizer(x)\n",
    "        z = adversary(y)\n",
    "\n",
    "#         epsilon2 = 0.0001\n",
    "#         previous_loss2 = 0\n",
    "        for k in range(n):\n",
    "        \n",
    "            _, a_loss = sess.run((ad_train, class_error))\n",
    "#             if (previous_loss2 - a_loss) <= epsilon2:\n",
    "#                 break\n",
    "#             previous_loss2 = a_loss\n",
    "\n",
    "        _, p_loss = sess.run((priv_train, priv_loss))\n",
    "        if abs(previous_loss1 - p_loss) < epsilon1:\n",
    "            break\n",
    "        previous_loss1 = p_loss\n",
    "\n",
    "        print(\"Iterations: %d\\t Adversary loss: %.4f\\t Privatizer loss: %.4f\"%(i, a_loss,p_loss))\n",
    "        f.write(\"Iterations: %d\\t Adversary loss: %.4f\\t Privatizer loss: %.4f\\n\"%(i, a_loss,p_loss))\n",
    "        \n",
    "    f.close()\n",
    "    os.system(\"printf '\\a'\")\n",
    "    \n",
    "    show_viz = False\n",
    "    if show_viz:\n",
    "        plot_rssi(x, y, sess)\n",
    "        plot_users(x, y, z, trueusers, sess)\n",
    "        _, inputmodel, outputmodel = signal_map_error(x, y)\n",
    "        plot_maps(inputmodel, outputmodel, sess)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Iterations: 0\t Adversary loss: 1.2998\t Privatizer loss: 5.3638\n",
      "Iterations: 1\t Adversary loss: 1.1674\t Privatizer loss: 0.2431\n",
      "Iterations: 2\t Adversary loss: 1.0744\t Privatizer loss: -0.4281\n",
      "Iterations: 3\t Adversary loss: 1.0004\t Privatizer loss: -0.5841\n",
      "Iterations: 4\t Adversary loss: 0.9412\t Privatizer loss: -0.6194\n",
      "Iterations: 5\t Adversary loss: 0.8941\t Privatizer loss: -0.6215\n",
      "Iterations: 6\t Adversary loss: 0.8564\t Privatizer loss: -0.6161\n",
      "Iterations: 7\t Adversary loss: 0.8257\t Privatizer loss: -0.6098\n",
      "Iterations: 8\t Adversary loss: 0.8003\t Privatizer loss: -0.6037\n",
      "Iterations: 9\t Adversary loss: 0.7788\t Privatizer loss: -0.5978\n",
      "Iterations: 10\t Adversary loss: 0.7586\t Privatizer loss: -0.5909\n",
      "Iterations: 11\t Adversary loss: 0.7428\t Privatizer loss: -0.5864\n",
      "Iterations: 12\t Adversary loss: 0.7301\t Privatizer loss: -0.5835\n",
      "Iterations: 13\t Adversary loss: 0.7198\t Privatizer loss: -0.5822\n",
      "Iterations: 14\t Adversary loss: 0.7127\t Privatizer loss: -0.5832\n",
      "Iterations: 15\t Adversary loss: 0.7084\t Privatizer loss: -0.5862\n",
      "Iterations: 16\t Adversary loss: 0.7063\t Privatizer loss: -0.5907\n",
      "Iterations: 17\t Adversary loss: 0.7067\t Privatizer loss: -0.5972\n",
      "Iterations: 18\t Adversary loss: 0.7098\t Privatizer loss: -0.6059\n",
      "Iterations: 19\t Adversary loss: 0.7156\t Privatizer loss: -0.6168\n",
      "Iterations: 20\t Adversary loss: 0.7236\t Privatizer loss: -0.6297\n",
      "Iterations: 21\t Adversary loss: 0.7324\t Privatizer loss: -0.6431\n",
      "Iterations: 22\t Adversary loss: 0.7414\t Privatizer loss: -0.6566\n",
      "Iterations: 23\t Adversary loss: 0.7492\t Privatizer loss: -0.6689\n",
      "Iterations: 24\t Adversary loss: 0.7550\t Privatizer loss: -0.6788\n",
      "Iterations: 25\t Adversary loss: 0.7589\t Privatizer loss: -0.6865\n",
      "Iterations: 26\t Adversary loss: 0.7605\t Privatizer loss: -0.6918\n",
      "Iterations: 27\t Adversary loss: 0.7603\t Privatizer loss: -0.6950\n",
      "Iterations: 28\t Adversary loss: 0.7589\t Privatizer loss: -0.6968\n",
      "Iterations: 29\t Adversary loss: 0.7572\t Privatizer loss: -0.6981\n",
      "Iterations: 30\t Adversary loss: 0.7550\t Privatizer loss: -0.6988\n",
      "Iterations: 31\t Adversary loss: 0.7534\t Privatizer loss: -0.6998\n",
      "Iterations: 32\t Adversary loss: 0.7526\t Privatizer loss: -0.7017\n",
      "Iterations: 33\t Adversary loss: 0.7529\t Privatizer loss: -0.7044\n",
      "Iterations: 34\t Adversary loss: 0.7542\t Privatizer loss: -0.7080\n",
      "Iterations: 35\t Adversary loss: 0.7564\t Privatizer loss: -0.7123\n",
      "Iterations: 36\t Adversary loss: 0.7593\t Privatizer loss: -0.7173\n",
      "Iterations: 37\t Adversary loss: 0.7630\t Privatizer loss: -0.7230\n",
      "Iterations: 38\t Adversary loss: 0.7675\t Privatizer loss: -0.7293\n",
      "Iterations: 39\t Adversary loss: 0.7734\t Privatizer loss: -0.7369\n",
      "Iterations: 40\t Adversary loss: 0.7804\t Privatizer loss: -0.7455\n",
      "Iterations: 41\t Adversary loss: 0.7884\t Privatizer loss: -0.7551\n",
      "Iterations: 42\t Adversary loss: 0.7978\t Privatizer loss: -0.7660\n",
      "Iterations: 43\t Adversary loss: 0.8082\t Privatizer loss: -0.7779\n",
      "Iterations: 44\t Adversary loss: 0.8195\t Privatizer loss: -0.7905\n",
      "Iterations: 45\t Adversary loss: 0.8323\t Privatizer loss: -0.8044\n",
      "Iterations: 46\t Adversary loss: 0.8460\t Privatizer loss: -0.8193\n",
      "Iterations: 47\t Adversary loss: 0.8610\t Privatizer loss: -0.8354\n",
      "Iterations: 48\t Adversary loss: 0.8773\t Privatizer loss: -0.8527\n",
      "Iterations: 49\t Adversary loss: 0.8945\t Privatizer loss: -0.8710\n",
      "Iterations: 50\t Adversary loss: 0.9126\t Privatizer loss: -0.8901\n",
      "Iterations: 51\t Adversary loss: 0.9309\t Privatizer loss: -0.9094\n",
      "Iterations: 52\t Adversary loss: 0.9499\t Privatizer loss: -0.9293\n",
      "Iterations: 53\t Adversary loss: 0.9682\t Privatizer loss: -0.9484\n",
      "Iterations: 54\t Adversary loss: 0.9812\t Privatizer loss: -0.9621\n",
      "Iterations: 55\t Adversary loss: 0.9881\t Privatizer loss: -0.9697\n",
      "Iterations: 56\t Adversary loss: 0.9936\t Privatizer loss: -0.9760\n",
      "Iterations: 57\t Adversary loss: 0.9998\t Privatizer loss: -0.9828\n",
      "Iterations: 58\t Adversary loss: 1.0062\t Privatizer loss: -0.9898\n",
      "Iterations: 59\t Adversary loss: 1.0128\t Privatizer loss: -0.9970\n",
      "Iterations: 60\t Adversary loss: 1.0195\t Privatizer loss: -1.0042\n",
      "Iterations: 61\t Adversary loss: 1.0244\t Privatizer loss: -1.0095\n",
      "Iterations: 62\t Adversary loss: 1.0214\t Privatizer loss: -1.0071\n",
      "Iterations: 63\t Adversary loss: 1.0217\t Privatizer loss: -1.0078\n",
      "Iterations: 64\t Adversary loss: 1.0209\t Privatizer loss: -1.0073\n",
      "Iterations: 65\t Adversary loss: 1.0193\t Privatizer loss: -1.0062\n",
      "Iterations: 66\t Adversary loss: 1.0181\t Privatizer loss: -1.0054\n",
      "Iterations: 67\t Adversary loss: 1.0188\t Privatizer loss: -1.0064\n",
      "Iterations: 68\t Adversary loss: 1.0191\t Privatizer loss: -1.0070\n",
      "Iterations: 69\t Adversary loss: 1.0199\t Privatizer loss: -1.0081\n",
      "Iterations: 70\t Adversary loss: 1.0208\t Privatizer loss: -1.0092\n",
      "Iterations: 71\t Adversary loss: 1.0214\t Privatizer loss: -1.0101\n",
      "Iterations: 72\t Adversary loss: 1.0218\t Privatizer loss: -1.0108\n",
      "Iterations: 73\t Adversary loss: 1.0222\t Privatizer loss: -1.0114\n",
      "Iterations: 74\t Adversary loss: 1.0230\t Privatizer loss: -1.0125\n",
      "Iterations: 75\t Adversary loss: 1.0237\t Privatizer loss: -1.0133\n",
      "Iterations: 76\t Adversary loss: 1.0224\t Privatizer loss: -1.0123\n",
      "Iterations: 77\t Adversary loss: 1.0198\t Privatizer loss: -1.0098\n",
      "Iterations: 78\t Adversary loss: 1.0173\t Privatizer loss: -1.0074\n",
      "Iterations: 79\t Adversary loss: 1.0144\t Privatizer loss: -1.0048\n",
      "Iterations: 80\t Adversary loss: 1.0128\t Privatizer loss: -1.0033\n",
      "Iterations: 81\t Adversary loss: 1.0110\t Privatizer loss: -1.0017\n",
      "Iterations: 82\t Adversary loss: 1.0072\t Privatizer loss: -0.9979\n",
      "Iterations: 83\t Adversary loss: 1.0042\t Privatizer loss: -0.9951\n",
      "Iterations: 84\t Adversary loss: 1.0018\t Privatizer loss: -0.9928\n",
      "Iterations: 85\t Adversary loss: 0.9996\t Privatizer loss: -0.9907\n",
      "Iterations: 86\t Adversary loss: 0.9977\t Privatizer loss: -0.9890\n",
      "Iterations: 87\t Adversary loss: 0.9964\t Privatizer loss: -0.9878\n",
      "Iterations: 88\t Adversary loss: 0.9955\t Privatizer loss: -0.9870\n",
      "Iterations: 89\t Adversary loss: 0.9951\t Privatizer loss: -0.9868\n",
      "Iterations: 90\t Adversary loss: 0.9952\t Privatizer loss: -0.9870\n",
      "Iterations: 91\t Adversary loss: 0.9953\t Privatizer loss: -0.9872\n",
      "Iterations: 92\t Adversary loss: 0.9956\t Privatizer loss: -0.9876\n",
      "Iterations: 93\t Adversary loss: 0.9960\t Privatizer loss: -0.9881\n",
      "Iterations: 94\t Adversary loss: 0.9971\t Privatizer loss: -0.9893\n",
      "Iterations: 95\t Adversary loss: 0.9983\t Privatizer loss: -0.9907\n",
      "Iterations: 96\t Adversary loss: 0.9989\t Privatizer loss: -0.9914\n",
      "Iterations: 97\t Adversary loss: 0.9995\t Privatizer loss: -0.9921\n",
      "Iterations: 98\t Adversary loss: 1.0004\t Privatizer loss: -0.9931\n",
      "Iterations: 99\t Adversary loss: 1.0013\t Privatizer loss: -0.9941\n"
     ]
    }
   ],
   "source": [
    "priv_nlayers = 2\n",
    "priv_nunits = 8\n",
    "ad_nlayers = 2\n",
    "ad_nunits = 8\n",
    "\n",
    "map_nlayers = 2 # keep\n",
    "map_nunits = 4 # keep\n",
    "map_epochs = 200 # keep\n",
    "penalty_value = 5 # keep\n",
    "threshold_value = 0.0001 # keep\n",
    "samplesize = 300 # keep\n",
    "n_iterations = 100 # keep\n",
    "\n",
    "scope_iter = 12 # REMOVE AFTER NEXT RUN\n",
    "\n",
    "privscope = \"priv\"+str(scope_iter)\n",
    "adscope = \"ad\"+str(scope_iter)\n",
    "\n",
    "print(scope_iter)\n",
    "train_GAP(privscope, adscope, priv_nlayers, priv_nunits, ad_nlayers, ad_nunits, map_nlayers, map_nunits, map_epochs, penalty_value, threshold_value, samplesize, n_iterations)\n",
    "\n",
    "scope_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyperparameters for map creation (in isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "10/10 [==============================] - 86s 9s/step - loss: 0.9529\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.9385\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.9249\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.9122\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8999\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8889\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8788\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8690\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8594\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8502\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8417\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8338\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8268\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8204\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8147\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.8095\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.8046\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8000\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7956\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7914\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7874\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7835\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7797\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7761\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7726\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7692\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7659\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.7627\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7596\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7567\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7538\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.7510\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.7483\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.7457\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7431\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7407\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7383\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7359\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7337\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7315\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7294\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7273\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7253\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7234\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7215\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7197\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7179\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7162\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7145\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7128\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7113\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7097\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7082\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7068\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7054\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7040\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7026\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7014\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7001\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6989\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6977\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6965\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6954\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6943\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6923\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6913\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6903\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6894\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6885\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6876\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6868\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6859\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6851\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6843\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6836\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6829\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6822\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6815\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6808\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6802\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6796\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6789\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6783\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6778\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6773\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6768\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6763\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6758\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6754\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6749\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6745\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6741\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6737\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6734\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.6730\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6726\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6723\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6719\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6716\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6713\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6710\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6708\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6705\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6702\n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6699\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6697\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6695\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6693\n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6691\n",
      "Epoch 111/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6689\n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6687\n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6685\n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6683\n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6681\n",
      "Epoch 116/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6680\n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6678\n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6677\n",
      "Epoch 119/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6675\n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6674\n",
      "Epoch 121/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6673\n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6672\n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6671\n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6669\n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6668\n",
      "Epoch 126/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6667\n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6666\n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6665\n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6664\n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6663\n",
      "Epoch 131/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6663\n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6662\n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6661\n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6660\n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6660\n",
      "Epoch 136/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6659\n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6658\n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6658\n",
      "Epoch 139/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6657\n",
      "Epoch 140/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6657\n",
      "Epoch 141/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6656\n",
      "Epoch 142/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6656\n",
      "Epoch 143/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6655\n",
      "Epoch 144/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6655\n",
      "Epoch 145/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6654\n",
      "Epoch 146/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6654\n",
      "Epoch 147/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6653\n",
      "Epoch 148/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6653\n",
      "Epoch 149/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6653\n",
      "Epoch 150/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6652\n",
      "Epoch 151/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6652\n",
      "Epoch 152/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6652\n",
      "Epoch 153/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6651\n",
      "Epoch 154/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6651\n",
      "Epoch 155/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6651\n",
      "Epoch 156/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6650\n",
      "Epoch 157/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6650\n",
      "Epoch 158/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6649\n",
      "Epoch 159/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6649\n",
      "Epoch 160/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6649\n",
      "Epoch 161/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6649\n",
      "Epoch 162/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6648\n",
      "Epoch 163/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6648\n",
      "Epoch 164/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6648\n",
      "Epoch 165/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6648\n",
      "Epoch 166/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6647\n",
      "Epoch 167/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6647\n",
      "Epoch 168/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6647\n",
      "Epoch 169/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6647\n",
      "Epoch 170/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6646\n",
      "Epoch 171/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6646\n",
      "Epoch 172/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6646\n",
      "Epoch 173/300\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model from input\n",
    "map_nlayers = 2\n",
    "map_nunits = 4\n",
    "map_epochs = 300\n",
    "steps = 10\n",
    "samplesize = 100\n",
    "\n",
    "f = open((\"train_GAP/MAP\"+str((map_nlayers, map_nunits, map_epochs, steps, samplesize))+\".txt\"), \"w\")\n",
    "f.write(\"%d layers with %d units in the map model\\n\"%(map_nlayers, map_nunits))\n",
    "f.write(\"trained for %d epochs with %d steps per epoch\\n\"%(map_epochs, steps))\n",
    "f.write(\"train on %d samples\\n\"%(samplesize))\n",
    "f.write(\"np.random.seed(0)\\n\\n\")\n",
    "\n",
    "x, trueusers = sampleChania(Chania, samplesize)\n",
    "\n",
    "inputmodel = keras.Sequential()\n",
    "inputmodel.add(keras.layers.Dense(map_nunits, input_shape=(2, ), activation=tf.nn.relu))\n",
    "for l in range(map_nlayers-1):\n",
    "    inputmodel.add(keras.layers.Dense(map_nunits, activation=tf.nn.relu))\n",
    "inputmodel.add(keras.layers.Dense(1))\n",
    "optimizer = tf.train.AdamOptimizer() #or FtrlOptimizer\n",
    "inputmodel.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        f.write(\"Epoch: %d\\t Loss: %.4f\\n\"%(epoch, logs.get('loss')))\n",
    "history = LossHistory()\n",
    "inputmodel.fit(x[:,2:], x[:,1], steps_per_epoch=steps, epochs=map_epochs, verbose=1, callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001), history])\n",
    "\n",
    "f.close()\n",
    "os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ad_146/dense_1/Softmax:0' shape=(100, 5) dtype=float32>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0' shape=(100,) dtype=float32>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.sparse_categorical_crossentropy(trueusers, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.sparse_categorical_crossentropy(trueusers, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
