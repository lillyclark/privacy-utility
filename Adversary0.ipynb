{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sys, os\n",
    "import itertools\n",
    "import numpy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in matlab data - this takes a while and about 2gb memory\n",
      "Done loading matlab data.\n"
     ]
    }
   ],
   "source": [
    "matlab_filename = 'realitymining.mat'\n",
    "print(\"Loading in matlab data - this takes a while and about 2gb memory\")\n",
    "matlab_obj = scipy.io.loadmat(matlab_filename)\n",
    "print(\"Done loading matlab data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validSubjects(allSubjects):\n",
    "    return [s for s in allSubjects if hasNumeric(s,'mac') and hasNumeric(s,'my_hashedNumber')]\n",
    "\n",
    "\n",
    "# idDicts: subjects -> {int: subject}, {float: (int, subject)}, {int: (int, subject)}\n",
    "# First hash is contiguousId: subjectObject\n",
    "# second hash is macAddress: contiguousId, subjectObject\n",
    "# third hash is hashedNumber: contiguousId, subjectObject\n",
    "# because the id dictionaries reference the subject object, we can replace\n",
    "# the array of subject objects with these dictionaries.\n",
    "\n",
    "def idDicts(subjects):\n",
    "    return (dict((i, s) for (i,s) in enumerate(subjects)),\n",
    "        dict((getNumeric(s,'mac'), (i, s)) for (i,s) in enumerate(subjects)),\n",
    "        dict((getNumeric(s, 'my_hashedNumber'), (i, s)) for (i,s) in enumerate(subjects)))\n",
    "\n",
    "def hasNumeric(obj, field):\n",
    "    try:\n",
    "        obj[field][0][0]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def getNumeric(obj, field):\n",
    "    return obj[field][0][0]\n",
    "\n",
    "def hasArray(obj, field):\n",
    "    try:\n",
    "        obj[field][0]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def getArray(obj, field):\n",
    "    return obj[field][0]\n",
    "\n",
    "def convertDatetime(dt):\n",
    "    return datetime.fromordinal(int(dt)) + timedelta(days=dt%1) - timedelta(days=366) - timedelta(hours=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting valid subjects and creating id dictionaries.\n"
     ]
    }
   ],
   "source": [
    "print('Extracting valid subjects and creating id dictionaries.')\n",
    "subjects = validSubjects(matlab_obj['s'][0])\n",
    "idDictionaries = idDicts(subjects)\n",
    "idDict, macDict, hashNumDict = idDictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime, area.cell -> userID\n",
    "adversaryData = []\n",
    "\n",
    "for subjectID, subject in idDict.items():\n",
    "    if hasArray(subject, 'locs'):\n",
    "        for event in subject['locs']:\n",
    "            try:\n",
    "                timeplace = list(event)\n",
    "#                 time = convertDatetime(timeplace[0])\n",
    "                time = timeplace[0]\n",
    "                place = timeplace[1]\n",
    "                # assumes two people aren't texting at the exact same time from the same place\n",
    "                if place != 0.0:\n",
    "                    adversaryData.append([time, place, subjectID])\n",
    "            except:\n",
    "                pass\n",
    "RNNinput = np.array(adversaryData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNinput = RNNinput[RNNinput[:,0].argsort()]\n",
    "extendedData = RNNinput\n",
    "LSTMData = RNNinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.31947157e+05, 5.11940811e+03, 8.70000000e+01],\n",
       "       [7.31947157e+05, 5.11940811e+03, 8.70000000e+01],\n",
       "       [7.31947157e+05, 5.11940811e+03, 8.70000000e+01],\n",
       "       [7.31964715e+05, 5.18860241e+03, 6.70000000e+01],\n",
       "       [7.31964715e+05, 5.18840811e+03, 6.70000000e+01],\n",
       "       [7.31964716e+05, 5.18840813e+03, 6.70000000e+01],\n",
       "       [7.31964716e+05, 5.18840811e+03, 6.70000000e+01],\n",
       "       [7.31964716e+05, 5.18842171e+03, 6.70000000e+01],\n",
       "       [7.31964717e+05, 5.18840811e+03, 6.70000000e+01],\n",
       "       [7.31964717e+05, 5.18840332e+03, 6.70000000e+01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNinput[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RNNinput[:,0:2]\n",
    "labels = RNNinput[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847313"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RNNinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (5800, 2)\n",
      "Testing set:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "N = 6000\n",
    "n = 5800\n",
    "train_data = data[:][0:n]\n",
    "train_labels = labels[:][0:n]\n",
    "test_data = data[:][n:N]\n",
    "test_labels = labels[:][n:N]\n",
    "print(\"Training set: {}\".format(train_data.shape))\n",
    "print(\"Testing set:  {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 448us/step\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(3, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(3, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(90, activation=tf.nn.softmax))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=300, verbose=0, batch_size=100)\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Location as a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loc = np.append(0, extendedData[0:len(extendedData)-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extendedData = np.column_stack((extendedData, last_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(extendedData)):\n",
    "    if i != 0:\n",
    "        if extendedData[i,2] != extendedData[i-1,2]:\n",
    "            extendedData[i, 3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((extendedData[:,0:2], extendedData[:,3]))\n",
    "labels = extendedData[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (4800, 3)\n",
      "Testing set:  (200, 3)\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "n = 4800\n",
    "train_data = data[:][0:n]\n",
    "train_labels = labels[:][0:n]\n",
    "test_data = data[:][n:N]\n",
    "test_labels = labels[:][n:N]\n",
    "print(\"Training set: {}\".format(train_data.shape))\n",
    "print(\"Testing set:  {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s 8ms/step\n",
      "0.025\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(90, activation=tf.nn.softmax))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=2000, verbose=0, batch_size=100)\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LSTMData[:,0:2]\n",
    "labels = LSTMData[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (4800, 2)\n",
      "Testing set:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "n = 4800\n",
    "train_data = data[:][0:n]\n",
    "train_labels = labels[:][0:n]\n",
    "test_data = data[:][n:N]\n",
    "test_labels = labels[:][n:N]\n",
    "print(\"Training set: {}\".format(train_data.shape))\n",
    "print(\"Testing set:  {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([731964.71611111,   5188.40811   ]), 67.0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6], train_labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(90, 10, input_length=2))\n",
    "model.add(keras.layers.LSTM(10, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(10, return_sequences=True))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(90)))\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "# model.fit(train_data, train_labels, epochs=200, verbose=0, batch_size=100)\n",
    "# test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not working RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, \n",
    "                 n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    # Layer 1\n",
    "    input_layer = keras.layers.Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "\n",
    "    # Layer 2\n",
    "    embedding_layer = keras.layers.Embedding(input_dim=n_input_nodes, \n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True, name='embedding_layer')(input_layer) #mask_zero=True will ignore padding\n",
    "    # Output shape = (batch_size, seq_input_len, n_embedding_nodes)\n",
    "\n",
    "    #Layer 3\n",
    "    lstm_layer1 = keras.layers.LSTM(n_hidden_nodes,\n",
    "                     return_sequences=True, #return hidden state for each word, not just last one\n",
    "                     stateful=stateful, name='hidden_layer1')(embedding_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 4\n",
    "    lstm_layer2 = keras.layers.LSTM(n_hidden_nodes,\n",
    "                     return_sequences=True,\n",
    "                     stateful=stateful, name='hidden_layer2')(lstm_layer1)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 5\n",
    "    output_layer = keras.layers.TimeDistributed(keras.layers.Dense(n_input_nodes, activation=\"softmax\"), \n",
    "                                   name='output_layer')(lstm_layer2)\n",
    "    # Output shape = (batch_size, seq_input_len, n_input_nodes)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3900, 2)\n",
      "Testing set:  (100, 2)\n"
     ]
    }
   ],
   "source": [
    "data = RNNinput[:,0:2]\n",
    "labels = RNNinput[:,2]\n",
    "N = 4000\n",
    "order = np.argsort(np.random.random(labels.shape))\n",
    "n = 3900\n",
    "train_data = data[order][0:n]\n",
    "train_labels = labels[order][0:n]\n",
    "train_labels = np.reshape(train_labels, (n, 1, 1))\n",
    "test_data = data[order][n:N]\n",
    "test_labels = labels[order][n:N]\n",
    "print(\"Training set: {}\".format(train_data.shape))\n",
    "print(\"Testing set:  {}\".format(test_data.shape))\n",
    "\n",
    "model = create_model(seq_input_len=2,\n",
    "                     n_input_nodes = 90,\n",
    "                     n_embedding_nodes = 300,\n",
    "                     n_hidden_nodes = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=train_data, y=train_labels, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Adversary, Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretically what I think the ML model should do\n",
    "<code>\n",
    "userdict : userid -> [[time, location]]\n",
    "userdict[0] = [train_data[0]]\n",
    "\n",
    "max_dist = estimate of how far away points ever are\n",
    "</code>\n",
    "\n",
    "#### Prediction\n",
    "<code>\n",
    "for point in train_data:\n",
    "    likelihood vector = zeros\n",
    "    for user in userdict:\n",
    "        ## This is the nonlinearity that needs to train\n",
    "        p = ((point.location - user.location) - max_dist/2)/(point.time - user.time)\n",
    "    if the max value of the likelihood vector is below a threshhold, and there are less than 90 users:\n",
    "        user[new] = [point]\n",
    "    else\n",
    "        guess = argmax(likelihood vector)\n",
    "        user[guess] = user[guess] + point\n",
    "</code>\n",
    "        \n",
    "#### Loss\n",
    "<code>\n",
    "for point in train_data:\n",
    "    if [point.time, point.location] not in userdict[point.user]:\n",
    "            loss += 1\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "inputdata = RNNinput[0:size]\n",
    "\n",
    "userdict = {}\n",
    "userdict[0] = [inputdata[0]]\n",
    "max_dist = 1\n",
    "threshold = -10000\n",
    "\n",
    "for point in inputdata:\n",
    "    l = np.zeros(len(userdict))\n",
    "    for u in userdict.keys():\n",
    "        deltaloc = abs(point[1] - userdict[u][-1][1])\n",
    "        deltat = point[0] - userdict[u][-1][0]\n",
    "        if deltat == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = -(deltaloc - max_dist/2)/(deltat)\n",
    "        l[u] = p\n",
    "    if max(l) < threshold and len(userdict) < 90:\n",
    "        userdict[max(userdict.keys())+1] = [point]\n",
    "    else:\n",
    "        guess = np.argmax(l)\n",
    "        userdict[guess] = np.vstack([userdict[guess], point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 969\n",
      "1 2449\n",
      "2 1576\n",
      "3 4684\n",
      "4 323\n"
     ]
    }
   ],
   "source": [
    "for u in userdict.keys():\n",
    "    print(u, len(userdict[u]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for point in inputdata:\n",
    "    if point.tolist() not in userdict[point[2]].tolist():\n",
    "        loss += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss/size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "# lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# # initial state of memory\n",
    "# state = lstm.zero_state(batch_size, dtype=tf.float32)\n",
    "# probabilities = []\n",
    "# loss = 0.0\n",
    "# output, state = lstm(words, state)\n",
    "# logits = lstm.matmul(output, softmax_w) + softmax_b\n",
    "# probabilities.append(tf.nn.softmax(logits))\n",
    "# loss += loss_function(probabilities, target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the tensorflow lstm used in the language processing example,\n",
    "I could recreate the data so that it's in the form\n",
    "[location, userID, location, userID, ....]\n",
    "where inputs are time ordered by the location time stamps.\n",
    "This way to estimate a user, input the location to the RNN and predict the next value.\n",
    "\n",
    "I should remove any loc=0.0 events since those are errors anyway.\n",
    "\n",
    "in an unsupervised setting, the adversary would start by assigning userID 0 to the first location data, and then userID 1 to the next location event that is more than some distance d away, etc, and feed that data into a recurrent nn that has \"other\" as one of the options. When \"other\" is the most likely, add a new userID and restart the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.array([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
